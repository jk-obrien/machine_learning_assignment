---
title: "Machine Learning: Course Project"
output: html_document
---

```{r init}
set.seed(77412)
library(caret, quietly=TRUE)
train <- read.csv("pml-training.csv")
```

# Summary
The data loaded above is a large set of `r format(dim(train)[1], big.mark=",", trim=TRUE)`
observations and `r dim(train)[2]` variables gathered from motion sensors attached to 
volunteers who were performing weight lifting exercises. Full details are
[here](http://groupware.les.inf.puc-rio.br/har). The aim of this project is to build a
model, using the random forest machine learning algorithm, that can predict the exercise
being performed from the motion sensor data.


## Data Cleaning
There are many instances of the string "#DIV/0!" and of the empty string "" in 
the data. Let's treat those as NA values.
```{r clean}
train[train==""]        <- NA
train[train=="#DIV/0!"] <- NA
```

Many of the columns are now almost completely filled with NAs. Make a function
to measure this and then a histogram to see how the proportions of NAs are
distributed.
```{r nas}
na_prop <- function(x) sum(is.na(x))/length(x)
nas     <- sapply(train, na_prop)
hist(nas, xlab="Proportion of NA values", main="Histogram of NA Proportions")
```

**Figure 1** Histogram showing the proportion of NA values in each variable
after data cleaning.

The histogram shows that the variables are divided into those with
90% or more NA values, and those with 10% or less - in fact closer inspection
of the latter group shows that none of them have any NA values. Continue the 
study using only this group of variables as predictors, but save the names of 
the dropped variables so we can also remove them from the test set later on.

```{r slim}
droppedVars <- names(nas[nas>0.9])
train       <- train[names(nas[nas==0])]
```

## Feature Selection
The first seven variables remaining in `train` are the row numbers, the
participants' names, and various types of time-stamps and window variables.
Inspection (not shown) of these indicates that most of them will not be of much
value for prediction of the `classe` variable. Even if they were, the aim of the
study is to use the motion sensor data for this purpose, so we will try to
rely only on those variables and remove the first seven variables also. Again, we 
add the column names to our list.
```{r seven}
droppedVars <- c(droppedVars, names(train)[1:7])
train       <- train[-1:-7]
```

Lastly, check the remaining variables for low variance and for high correlations
among themselves.
```{r varCor}
nearZeroVar(train)                          # No low variance columns found.

corMat      <- cor(train[-dim(train)[2]])
hiCor       <- findCorrelation(corMat)      # 7 highly correlated columns found.
droppedVars <- c(droppedVars, names(train)[hiCor])
train       <- train[-hiCor]
```

## Data Transformation
Have a look at the data that we have left for unusual distributions, skew, etc.
```{r inspect, warning=FALSE, message=FALSE, fig.height=8, fig.width=9}
library(reshape2)
hist_data <- melt(train[,-46])
ggplot(hist_data, aes(x=value)) +
    facet_wrap(~variable, scales="free_x") +
    geom_histogram()
```

**Figure 2** Histograms of remaining variables in the training set.

There seems to be a lot of skew in some variables (e.g. gryos_dumbbell_y) but
if we use the random forest algorithm, our approach will not be sensitive to 
skew and, as there seem to be no other obvious problems, we will not apply any
transformations to the data.

## Data Slicing
Divide the training set in two - one set (70%) for training, and another (30%)
for cross validation. Testing will be performed on a further data set which has
not yet been loaded.
```{r slice}
divInd <- createDataPartition(y=train$classe, p=0.7, list=FALSE)
tset   <- train[ divInd,]
vset   <- train[-divInd,]
```

## Model Building
```{r rflib, echo=FALSE, message=FALSE}
library(randomForest)
```

Now we are ready to carry out the training. After some initial experimentation
(not shown here) we will use random forests.
```{r forest, cache=TRUE, message=FALSE}
rfMod <- train(
    y=tset$classe, x=tset[,-46], allowParallel=TRUE,
    trControl = trainControl(method = "cv", number = 5)
)
rfMod$finalModel
```

## Out of Sample Error
The out of bag (OOB) error is very low at 
`r round(rfMod$finalModel$err.rate[500,1]*100,4)`%. It has been argued, for example
[here](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr),
that the OOB error provides an unbiased estimate of the test set error, while
others, for example [here](http://www.scirp.org/journal/PaperDownload.aspx?paperID=8072),
have argued that this is not always the case. We will take no position in this
debate and will supply both the OOB and out of sample (OOS) estimates.

We estimate our out of sample error using cross-validation against the random
subssample, `vset`, which we set aside earlier.
```{r validate}
predV   <- predict(rfMod, vset)
correct <- predV==vset$classe
oos     <- 1 - sum(correct)/length(correct)
table(predV, vset$classe)
```

This gives us a value of `r round(oos*100, 4)`% for our OOS error estimate,
which is a little higher than the OOB error, but still very small.

## Prediction against Test Set
Finally, load the test data, remove the same columns we removed from the training data, and
then make our predictions to submit for the second part of the assignment.

```{r test}
test  <- read.csv("pml-testing.csv")
test  <- test[ , -which(names(test) %in% droppedVars) ]
predT <- predict(rfMod, test[,-46])
```
